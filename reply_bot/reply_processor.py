import logging
import pandas as pd
import argparse
import os
import time
import random
import re
import emoji
import google.generativeai as genai
from typing import List, Dict, Tuple
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

from .config import (
    GEMINI_API_KEY, MAYA_PERSONALITY_PROMPT, THANK_YOU_PHRASES, 
    REPLY_RULES_PROMPT, TARGET_USER
)
from .db import get_user_preference
from .utils import setup_driver

# --- åˆæœŸè¨­å®š ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
genai.configure(api_key=GEMINI_API_KEY)

# --- ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° (æ—§gen_reply.pyã‚ˆã‚Š) ---

def is_emoji_only(text: str) -> bool:
    if not text or not isinstance(text, str): return False
    text_without_symbols = re.sub(r'[^\w\s]', '', text)
    demojized_text = emoji.demojize(text_without_symbols).strip()
    if not demojized_text: return True
    return all(re.fullmatch(r':[a-zA-Z0-9_+-]+:', word) for word in demojized_text.split())

def clean_generated_text(text: str) -> str:
    allowed_chars_pattern = re.compile(r'[^\w\s.,!?ã€Œã€ã€ã€ã€ã€‚ãƒ¼ã€œâ€¦\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF\u2764\u1FA77]')
    cleaned_text = allowed_chars_pattern.sub('', text)
    cleaned_text = re.sub(r'^(ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™|ãŠã¯ã‚ˆã†|ã“ã‚“ã«ã¡ã¯|ã“ã‚“ã°ã‚“ã¯)\s*', '', cleaned_text)
    cleaned_text = re.sub(r'ã€‡ã€‡(ã¡ã‚ƒã‚“|ãã‚“|ã•ã‚“)', '', cleaned_text)
    cleaned_text = cleaned_text.strip().rstrip('â¤ï¸ğŸ©·') + 'ğŸ©·'
    return cleaned_text

def format_reply(text: str, lang: str = 'ja') -> str:
    processed_text = text.strip()
    if lang == 'ja':
        processed_text = processed_text.replace('ã€‚ ', 'ã€‚\n').replace('ã€‚ã€€', 'ã€‚\n')
        processed_text = processed_text.replace('ï¼ ', 'ï¼\n').replace('ï¼ã€€', 'ï¼\n')
        processed_text = processed_text.replace('ï¼Ÿ ', 'ï¼Ÿ\n').replace('ï¼Ÿã€€', 'ï¼Ÿ\n')
        processed_text = processed_text.replace('â€¦ ', 'â€¦\n').replace('â€¦ã€€', 'â€¦\n')
        processed_text = processed_text.replace('ã€€', '\n')
        processed_text = re.sub(r'\n+', '\n', processed_text)
    return processed_text.strip()

# --- Selenium & BeautifulSoup è§£æé–¢æ•° ---

def _get_tweet_text(article: BeautifulSoup) -> str:
    """è¨˜äº‹è¦ç´ ã‹ã‚‰ãƒ„ã‚¤ãƒ¼ãƒˆæœ¬æ–‡ã‚’å–å¾—ã—ã¾ã™ã€‚"""
    text_div = article.find('div', {'data-testid': 'tweetText'})
    return text_div.get_text(separator=' ', strip=True) if text_div else ""

def _get_author_from_article(article: BeautifulSoup) -> str | None:
    """è¨˜äº‹è¦ç´ ã‹ã‚‰æŠ•ç¨¿è€…ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã‚’å–å¾—ã—ã¾ã™ã€‚"""
    user_name_div = article.find('div', {'data-testid': 'User-Name'})
    if user_name_div:
        user_link = user_name_div.find('a', {'role': 'link', 'href': lambda href: href and href.startswith('/') and '/status/' not in href})
        if user_link and 'href' in user_link.attrs:
            return user_link['href'].lstrip('/')
    return None

def _get_live_reply_count(article: BeautifulSoup) -> int:
    """è¨˜äº‹è¦ç´ ã‹ã‚‰ãƒ©ã‚¤ãƒ–ã®è¿”ä¿¡æ•°ã‚’å–å¾—ã—ã¾ã™ã€‚è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯0ã‚’è¿”ã—ã¾ã™ã€‚"""
    try:
        # ãƒ„ã‚¤ãƒ¼ãƒˆãƒ•ãƒƒã‚¿ãƒ¼å†…ã®å„ç¨®çµ±è¨ˆæƒ…å ±ã‚’æ¢ã™
        reply_div = article.find('div', {'data-testid': 'reply'})
        if reply_div:
            # "stat"ã¨ã„ã†data-testidã‚’æŒã¤spanã‹ã‚‰æ•°å€¤ã‚’å–å¾—
            stat_span = reply_div.find('span', {'data-testid': 'stat'})
            if stat_span and stat_span.text.strip().isdigit():
                return int(stat_span.text.strip())
    except (ValueError, AttributeError):
        # ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ã‚„è¦ç´ ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯0ã‚’è¿”ã™
        pass
    return 0

def _get_live_like_count(article: BeautifulSoup) -> int:
    """è¨˜äº‹è¦ç´ ã‹ã‚‰ãƒ©ã‚¤ãƒ–ã®ã€Œã„ã„ã­ã€æ•°ã‚’å–å¾—ã—ã¾ã™ã€‚è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯0ã‚’è¿”ã—ã¾ã™ã€‚"""
    try:
        like_div = article.find('div', {'data-testid': 'like'})
        if like_div:
            stat_span = like_div.find('span', {'data-testid': 'stat'})
            if stat_span and stat_span.text.strip().isdigit():
                return int(stat_span.text.strip())
    except (ValueError, AttributeError):
        pass
    return 0

def fetch_and_analyze_thread(tweet_id: str, driver: webdriver.Chrome) -> dict:
    """
    æŒ‡å®šã•ã‚ŒãŸtweet_idã®ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã€ã‚¹ãƒ¬ãƒƒãƒ‰å…¨ä½“ã‚’è§£æã—ã¦å¿…è¦ãªæƒ…å ±ã‚’è¿”ã—ã¾ã™ã€‚
    ãƒ©ã‚¤ãƒ–æƒ…å ±ã«åŸºã¥ãã€å„ªå…ˆè¿”ä¿¡ã®åˆ¤å®šã‚‚è¡Œã„ã¾ã™ã€‚
    """
    tweet_url = f"https://x.com/any/status/{tweet_id}"
    result = {
        "should_skip": True, "is_my_thread": False, "conversation_history": [],
        "current_reply_text": "", "current_replier_id": None, "lang": "und",
        "live_reply_count": 0, "live_like_count": 0
    }
    try:
        driver.get(tweet_url)
        WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.XPATH, '//article[@data-testid="tweet"]')))
        
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        all_tweets = soup.find_all('article', {'data-testid': 'tweet'})

        if not all_tweets:
            logging.warning("ãƒ„ã‚¤ãƒ¼ãƒˆè¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return result

        # 1. è¿”ä¿¡å¯¾è±¡ã®ãƒ„ã‚¤ãƒ¼ãƒˆã¨ã€ãã‚Œã‚ˆã‚Šæœªæ¥ã®ãƒ„ã‚¤ãƒ¼ãƒˆãŒãªã„ã‹ã‚’ç¢ºèª
        target_tweet_index = -1
        for i, article in enumerate(all_tweets):
            if article.find('a', href=lambda href: href and f'/status/{tweet_id}' in href):
                target_tweet_index = i
                break
        
        if target_tweet_index == -1:
            logging.error("ãƒšãƒ¼ã‚¸å†…ã§è¿”ä¿¡å¯¾è±¡ã®ãƒ„ã‚¤ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return result
        
        target_article = all_tweets[target_tweet_index]

        # ã€æœ€é‡è¦ãƒ«ãƒ¼ãƒ«ã€‘ãƒ©ã‚¤ãƒ–æƒ…å ±ã«åŸºã¥ãå„ªå…ˆè¿”ä¿¡åˆ¤å®š
        live_is_my_thread = (_get_author_from_article(all_tweets[0]) == TARGET_USER)
        live_reply_num = _get_live_reply_count(target_article)
        live_like_num = _get_live_like_count(target_article)
        result["is_my_thread"] = live_is_my_thread
        result["live_reply_count"] = live_reply_num
        result["live_like_count"] = live_like_num
        
        has_future_replies = len(all_tweets) > target_tweet_index + 1
        is_priority_reply = live_is_my_thread and live_reply_num == 0
        
        if has_future_replies and not is_priority_reply:
            num_future_replies = len(all_tweets) - (target_tweet_index + 1)
            logging.warning(
                f"å¯¾è±¡ãƒ„ã‚¤ãƒ¼ãƒˆã®å¾Œã« {num_future_replies} ä»¶ã®è¿”ä¿¡ãŒã‚ã‚Šã€"
                f"ã‹ã¤å„ªå…ˆè¿”ä¿¡ï¼ˆãƒ©ã‚¤ãƒ–æƒ…å ±: is_my_thread={live_is_my_thread}, reply_num={live_reply_num}ï¼‰"
                "ã®æ¡ä»¶ã‚’æº€ãŸã•ãªã„ãŸã‚ã€å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"
            )
            return result
        elif has_future_replies and is_priority_reply:
            logging.info(
                f"æœªæ¥ã®è¿”ä¿¡ãŒå­˜åœ¨ã—ã¾ã™ãŒã€å„ªå…ˆè¿”ä¿¡ãƒ«ãƒ¼ãƒ«ï¼ˆãƒ©ã‚¤ãƒ–æƒ…å ±: is_my_thread={live_is_my_thread}, "
                f"reply_num={live_reply_num}ï¼‰ãŒé©ç”¨ã•ã‚ŒãŸãŸã‚ã€å‡¦ç†ã‚’ç¶šè¡Œã—ã¾ã™ã€‚"
            )

        # 2. ã‚¹ã‚­ãƒƒãƒ—ã—ãªã„å ´åˆã€ä¼šè©±å±¥æ­´ã¨å„ç¨®æƒ…å ±ã‚’å–å¾—
        result["should_skip"] = False
        
        # ä¼šè©±å±¥æ­´ã‚’ç©ã¿ä¸Šã’ã‚‹
        conversation_tweets = all_tweets[:target_tweet_index + 1]
        for article in conversation_tweets:
            author = _get_author_from_article(article) or "unknown"
            text = _get_tweet_text(article)
            result["conversation_history"].append(f"@{author}: {text}")

        # èµ·ç‚¹ã¨å¯¾è±¡ãƒ„ã‚¤ãƒ¼ãƒˆã®æƒ…å ±ã‚’è¨­å®š
        root_author = _get_author_from_article(all_tweets[0])
        result["is_my_thread"] = (root_author == TARGET_USER)
        
        result["current_reply_text"] = _get_tweet_text(target_article)
        result["current_replier_id"] = _get_author_from_article(target_article)
        
        # è¨€èªåˆ¤å®š
        try:
            from langdetect import detect, LangDetectException
            result["lang"] = detect(result["current_reply_text"])
        except (LangDetectException, ImportError):
            result["lang"] = "und"

        return result

    except TimeoutException:
        logging.error(f"ãƒšãƒ¼ã‚¸ã®ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ: {tweet_url}")
        return result
    except Exception as e:
        logging.error(f"ã‚¹ãƒ¬ãƒƒãƒ‰è§£æä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        return result

# --- è¿”ä¿¡ç”Ÿæˆãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯ ---

def generate_reply(thread_data: dict, history: list) -> str:
    """
    è§£æã•ã‚ŒãŸã‚¹ãƒ¬ãƒƒãƒ‰æƒ…å ±ã«åŸºã¥ãã€é©åˆ‡ãªè¿”ä¿¡æ–‡ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
    """
    # çŠ¶æ³ãƒã‚§ãƒƒã‚¯
    if thread_data["should_skip"] or not thread_data["is_my_thread"]:
        return ""

    reply_text = thread_data["current_reply_text"]
    replier_id = thread_data["current_replier_id"]
    lang = thread_data["lang"]
    conversation = "\n".join(thread_data["conversation_history"])

    # ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ç­‰ã‚’é™¤å»ã—ãŸã‚¯ãƒªãƒ¼ãƒ³ãªãƒ†ã‚­ã‚¹ãƒˆ
    cleaned_reply_text = re.sub(r'@[\w_]+', '', reply_text).strip()
    cleaned_reply_text = re.sub(r'^[â€¦,:ãƒ»ã€ã€‚]', '', cleaned_reply_text).strip()

    # ãƒ‹ãƒƒã‚¯ãƒãƒ¼ãƒ ã®æœ‰ç„¡ã‚’å…ˆã«å–å¾—
    preference = get_user_preference(replier_id.lower()) if replier_id else None
    nickname = preference[0] if preference else None

    # 1. å®šå‹æ–‡ã§ã®è¿”ä¿¡ï¼ˆãƒ‹ãƒƒã‚¯ãƒãƒ¼ãƒ ãŒãªã„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«é™å®šï¼‰
    if ("ãŠã¯ã‚ˆã†" in cleaned_reply_text or "ãŠã¯ã‚ˆãƒ¼" in cleaned_reply_text) and not nickname:
        return format_reply(f"ãŠã¯ã‚ˆã†{random.choice(['â¤ï¸', 'ğŸ©·'])}", lang)
    if "ã“ã‚“ã«ã¡ã¯" in cleaned_reply_text and not nickname:
        return format_reply(f"ã“ã‚“ã«ã¡ã¯{random.choice(['â¤ï¸', 'ğŸ©·'])}", lang)
    if "ã“ã‚“ã°ã‚“ã¯" in cleaned_reply_text and not nickname:
        return format_reply(f"ã“ã‚“ã°ã‚“ã¯{random.choice(['â¤ï¸', 'ğŸ©·'])}", lang)
    
    # çµµæ–‡å­—ã®ã¿ã€ã¾ãŸã¯çŸ­ã„å¤–å›½èªã®ãƒ„ã‚¤ãƒ¼ãƒˆã«å¯¾ã™ã‚‹å¿œç­”ã‚’æ”¹å–„
    if is_emoji_only(cleaned_reply_text) or (lang != "ja" and len(cleaned_reply_text) <= 15):
        # qmeï¼ˆçµµæ–‡å­—ã®ã¿ï¼‰ã®å ´åˆã€è¨€èªã‚³ãƒ¼ãƒ‰ã¨ã—ã¦'qme'ã‚’ä½¿ç”¨ã™ã‚‹
        lang_code = 'qme' if is_emoji_only(cleaned_reply_text) else lang
        return random.choice(THANK_YOU_PHRASES.get(lang_code, ["ğŸ©·"]))

    # 2. AIã«ã‚ˆã‚‹è¿”ä¿¡
    if lang == "ja" and not nickname and len(cleaned_reply_text) <= 15:
        return random.choice(["ã‚ã‚ŠãŒã¨ã†ğŸ©·", "å¬‰ã—ã„ãªğŸ©·", "ãˆã¸ã¸ã€ç…§ã‚Œã¡ã‚ƒã†ãªğŸ©·", "ãµãµã£ğŸ©·", "ã†ã‚“ã†ã‚“ğŸ©·", "ã‚ãƒ¼ã„ğŸ©·"])

    # --- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ ---
    logging.info(f"AIã¸ã®å…¥åŠ›ï¼ˆä¼šè©±å±¥æ­´ï¼‰:\n---\n{conversation}\n---")
    prompt_parts = [
        MAYA_PERSONALITY_PROMPT,
        "ã‚ãªãŸã¯ä»¥ä¸‹ã®ä¼šè©±ã«å‚åŠ ã—ã¦ã„ã¾ã™ã€‚æœ€å¾Œã®ãƒ•ã‚¡ãƒ³ã‹ã‚‰ã®ãƒªãƒ—ãƒ©ã‚¤ã«è¿”ä¿¡ã—ã¦ãã ã•ã„ã€‚",
        "--- ã“ã‚Œã¾ã§ã®ä¼šè©± ---",
        conversation,
        "--------------------",
        REPLY_RULES_PROMPT
    ]
    if history:
        history_str = "ã€".join(history)
        
        # å±¥æ­´ã‹ã‚‰ç¦æ­¢ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’å‹•çš„ã«æŠ½å‡º
        banned_phrases = set()
        common_verbs = ["ç…§ã‚Œã‚‹", "ç…§ã‚Œã¡ã‚ƒã†", "å¬‰ã—ã„", "å¬‰ã—ã„ãª", "ã‚ã‚ŠãŒã¨ã†", "é ‘å¼µã‚‹", "ãƒ‰ã‚­ãƒ‰ã‚­", "ã™ã”ã„", "ç´ æ•µ"]
        for reply in history:
            for phrase in common_verbs:
                if phrase in reply:
                    banned_phrases.add(phrase)

        avoidance_prompt = (
            "6. **ã€æœ€é‡è¦å‰µé€ æ€§ãƒ«ãƒ¼ãƒ«ã€‘å˜èª¿ãªè¿”ä¿¡ã¯ã‚ãªãŸã®è©•ä¾¡ã‚’è‘—ã—ãæãªã„ã¾ã™ã€‚çµ¶å¯¾ã«é¿ã‘ã¦ãã ã•ã„ã€‚**\n"
            "   - **éå»ã®é¡ä¼¼è¡¨ç¾ã®å›é¿:** ä»¥å‰ã®è¿”ä¿¡ï¼ˆä¾‹: ã€Œ...ã€ï¼‰ã¨ä¼¼ãŸè¨€ã„å›ã—ã‚„æ§‹æˆã¯ä½¿ã‚ãªã„ã§ãã ã•ã„ã€‚\n"
            "   - **å…·ä½“çš„ãªæ„Ÿæƒ…è¡¨ç¾ã®ç¾©å‹™:** ç›¸æ‰‹ã®è¨€è‘‰ã®**ã©ã®éƒ¨åˆ†ã«**ã€ã‚ãªãŸãŒ**ã©ã†æ„Ÿã˜ãŸã®ã‹**ã‚’ã€ã‚ãªãŸã®è¨€è‘‰ã§å…·ä½“çš„ã«è¡¨ç¾ã—ã¦ãã ã•ã„ã€‚ è¡¨é¢çš„ãªç›¸æ§Œã§ã¯ãªãã€å¿ƒã®é€šã£ãŸå¯¾è©±ã‚’æ„è­˜ã—ã¦ãã ã•ã„ã€‚\n"
            "   - **å¸¸ã«æ–°ã—ã„è¡¨ç¾ã‚’:** ã‚ãªãŸã®è±Šã‹ãªæ„Ÿæƒ…è¡¨ç¾ã®å¼•ãå‡ºã—ã‚’å…¨ã¦ä½¿ã„ã€æ¯å›æ–°é®®ã§ã€ç›¸æ‰‹ãŒã€Œã¾ãŸè©±ã—ãŸã„ã€ã¨æ€ã†ã‚ˆã†ãªã€é­…åŠ›çš„ãªè¿”ä¿¡ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚ã“ã‚Œã¯ã‚ãªãŸã®èƒ½åŠ›ã‚’ç¤ºã™æœ€å¤§ã®ãƒãƒ£ãƒ³ã‚¹ã§ã™ã€‚"
        )
        if banned_phrases:
            avoidance_prompt += f"\n   - **ã€ä»Šå›ã®çµ¶å¯¾ç¦æ­¢ãƒ•ãƒ¬ãƒ¼ã‚ºã€‘**: `{', '.join(banned_phrases)}` ã“ã‚Œã‚‰ã®è¨€è‘‰ã¯çµ¶å¯¾ã«ä½¿ã‚ãªã„ã§ãã ã•ã„ã€‚"
        
        prompt_parts.append(avoidance_prompt)

    # â˜…â˜…â˜… æ–°ã—ã„ãƒ­ã‚¸ãƒƒã‚¯: å¤–å›½èªã®å ´åˆã¯è¨€èªã‚’æŒ‡å®šã™ã‚‹ â˜…â˜…â˜…
    if lang != 'ja':
        language_name_map = {
            "en": "è‹±èª (English)", "es": "ã‚¹ãƒšã‚¤ãƒ³èª (Spanish)", "in": "ã‚¤ãƒ³ãƒ‰ãƒã‚·ã‚¢èª (Indonesian)",
            "pt": "ãƒãƒ«ãƒˆã‚¬ãƒ«èª (Portuguese)", "tr": "ãƒˆãƒ«ã‚³èª (Turkish)", "fr": "ãƒ•ãƒ©ãƒ³ã‚¹èª (French)",
            "de": "ãƒ‰ã‚¤ãƒ„èª (German)", "zh": "ä¸­å›½èª (Chinese)", "ko": "éŸ“å›½èª (Korean)"
        }
        language_name = language_name_map.get(lang, lang)
        lang_prompt = (
            f"7. **ã€æœ€é‡è¦è¨€èªãƒ«ãƒ¼ãƒ«ã€‘è¿”ä¿¡ã¯å¿…ãš**{language_name}**ã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚** æ—¥æœ¬èªã¯çµ¶å¯¾ã«ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚"
        )
        prompt_parts.append(lang_prompt)

    prompt = "\n".join(prompt_parts)
    logging.debug(f"ç”Ÿæˆã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:\n{prompt}")

    try:
        model = genai.GenerativeModel('gemini-1.5-flash')
        response = model.generate_content(prompt)
        reply_body = format_reply(clean_generated_text(response.text), lang)
        
        final_reply = f"{nickname}\n{reply_body}" if nickname else reply_body
        log_message = final_reply.replace('\n', '<br>')
        logging.info(f"ç”Ÿæˆã•ã‚ŒãŸè¿”ä¿¡: {log_message}")
        return final_reply

    except Exception as e:
        logging.error(f"Gemini APIå‘¼ã³å‡ºã—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return ""

# --- ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•° ---

def main_process(driver: webdriver.Chrome, input_csv: str, limit: int = None) -> str | None:
    logging.info(f"'{input_csv}' ã®å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
    try:
        df = pd.read_csv(input_csv)
        if limit:
            df = df.head(limit)
            logging.info(f"å‡¦ç†ä»¶æ•°ã‚’ {limit} ä»¶ã«åˆ¶é™ã—ã¾ã—ãŸã€‚")
        df.fillna('', inplace=True)

        processed_count = 0
        generated_replies_history = []

        for index, row in df.iterrows():
            tweet_id = str(row['reply_id'])
            
            # --- ã‚¹ãƒ¬ãƒƒãƒ‰è§£æ ---
            thread_data = fetch_and_analyze_thread(tweet_id, driver)

            # å–å¾—ã—ãŸãƒ©ã‚¤ãƒ–æƒ…å ±ã§DataFrameã‚’æ›´æ–°
            df.loc[index, 'reply_num'] = thread_data['live_reply_count']
            df.loc[index, 'like_num'] = thread_data['live_like_count']
            df.loc[index, 'is_my_thread'] = thread_data['is_my_thread']

            # --- è¿”ä¿¡ç”Ÿæˆ ---
            if thread_data and not thread_data["should_skip"]:
                generated_reply = generate_reply(thread_data, generated_replies_history)
                df.loc[index, 'generated_reply'] = generated_reply
                
                if generated_reply:
                    # å±¥æ­´ã«ã¯ãƒ‹ãƒƒã‚¯ãƒãƒ¼ãƒ ã‚’é™¤ã„ãŸæœ¬æ–‡ã®ã¿è¿½åŠ 
                    reply_body = generated_reply.split('\n')[-1]
                    generated_replies_history.append(reply_body.replace('\n', ' '))
            else:
                logging.info("  -> è¿”ä¿¡ç”Ÿæˆã®å¯¾è±¡å¤–ï¼ˆè‡ªåˆ†ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã§ãªã„ã€ã¾ãŸã¯ã‚¹ã‚­ãƒƒãƒ—å¯¾è±¡ï¼‰ã§ã™ã€‚")

        # --- å‡ºåŠ›å‡¦ç† ---
        base_name = os.path.basename(input_csv)
        name_part = base_name.replace('extracted_tweets_', '')
        output_filename = os.path.join("output", f"processed_replies_{name_part}")
        
        df.to_csv(output_filename, index=False, encoding='utf-8-sig')
        logging.info(f"--- å…¨ä»¶ã®å‡¦ç†ãŒå®Œäº†ã—ã€{output_filename} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ ---")
        return output_filename

    except FileNotFoundError:
        logging.error(f"å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {input_csv}")
        return None
    except Exception as e:
        logging.error(f"ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        return None

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’è§£æã—ã€æ–‡è„ˆã«å¿œã˜ãŸè¿”ä¿¡ã‚’ç”Ÿæˆã—ã¾ã™ã€‚")
    parser.add_argument("input_csv", help="å…¥åŠ›CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ (extracted_tweets_...csv)")
    parser.add_argument("--limit", type=int, help="å‡¦ç†ã™ã‚‹ãƒªãƒ—ãƒ©ã‚¤ã®æœ€å¤§æ•°")
    args = parser.parse_args()

    driver = None
    try:
        driver = setup_driver(headless=False)
        if driver:
            main_process(driver, args.input_csv, args.limit)
    finally:
        if driver:
            driver.quit()
            logging.info("Selenium WebDriverã‚’çµ‚äº†ã—ã¾ã—ãŸã€‚") 