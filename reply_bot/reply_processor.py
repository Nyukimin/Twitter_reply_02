import logging
import pandas as pd
import argparse
import os
import time
import random
import re
import emoji
import google.generativeai as genai
from typing import List, Dict, Tuple
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

from .config import (
    GEMINI_API_KEY, MAYA_PERSONALITY_PROMPT, THANK_YOU_PHRASES, 
    REPLY_RULES_PROMPT, TARGET_USER, GEMINI_MODEL_NAME
)
from .db import get_user_preference
from .utils import setup_driver

# --- åˆæœŸè¨­å®š ---
import json
from datetime import datetime

# ãƒ‡ãƒãƒƒã‚°ç”¨ã®ãƒ­ã‚¬ãƒ¼è¨­å®š
import os
os.makedirs('log', exist_ok=True)  # logãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ

logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
genai.configure(api_key=GEMINI_API_KEY)

# è¤‡æ•°ã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’è¨­å®š
# 1. ã‚¹ãƒ¬ãƒƒãƒ‰è§£æãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°
thread_debug_logger = logging.getLogger('thread_debug')
thread_debug_handler = logging.FileHandler('log/thread_debug.log', encoding='utf-8')
thread_debug_handler.setLevel(logging.DEBUG)
thread_debug_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
thread_debug_handler.setFormatter(thread_debug_formatter)
thread_debug_logger.addHandler(thread_debug_handler)
thread_debug_logger.setLevel(logging.DEBUG)

# 2. is_replyåˆ¤å®šå°‚ç”¨ãƒ­ã‚°
reply_judge_logger = logging.getLogger('reply_judge')
reply_judge_handler = logging.FileHandler('log/reply_judgment.log', encoding='utf-8')
reply_judge_handler.setLevel(logging.DEBUG)
reply_judge_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
reply_judge_handler.setFormatter(reply_judge_formatter)
reply_judge_logger.addHandler(reply_judge_handler)
reply_judge_logger.setLevel(logging.DEBUG)

# 3. ã‚¹ãƒ¬ãƒƒãƒ‰ä¸»åˆ¤å®šå°‚ç”¨ãƒ­ã‚°
thread_owner_logger = logging.getLogger('thread_owner')
thread_owner_handler = logging.FileHandler('log/thread_owner_judgment.log', encoding='utf-8')
thread_owner_handler.setLevel(logging.DEBUG)
thread_owner_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
thread_owner_handler.setFormatter(thread_owner_formatter)
thread_owner_logger.addHandler(thread_owner_handler)
thread_owner_logger.setLevel(logging.DEBUG)

# 4. ç·åˆå‡¦ç†ãƒ­ã‚°
process_logger = logging.getLogger('process')
process_handler = logging.FileHandler('log/main_process.log', encoding='utf-8')
process_handler.setLevel(logging.INFO)
process_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
process_handler.setFormatter(process_formatter)
process_logger.addHandler(process_handler)
process_logger.setLevel(logging.INFO)

# --- ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° (æ—§gen_reply.pyã‚ˆã‚Š) ---

def is_emoji_only(text: str) -> bool:
    if not text or not isinstance(text, str): return False
    text_without_symbols = re.sub(r'[^\w\s]', '', text)
    demojized_text = emoji.demojize(text_without_symbols).strip()
    if not demojized_text: return True
    return all(re.fullmatch(r':[a-zA-Z0-9_+-]+:', word) for word in demojized_text.split())

def clean_generated_text(text: str) -> str:
    allowed_chars_pattern = re.compile(r'[^\w\s.,!?ã€Œã€ã€ã€ã€ã€‚ãƒ¼ã€œâ€¦\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF\u2764\u1FA77]')
    cleaned_text = allowed_chars_pattern.sub('', text)
    cleaned_text = re.sub(r'^(ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™|ãŠã¯ã‚ˆã†|ã“ã‚“ã«ã¡ã¯|ã“ã‚“ã°ã‚“ã¯)\s*', '', cleaned_text)
    cleaned_text = re.sub(r'ã€‡ã€‡(ã¡ã‚ƒã‚“|ãã‚“|ã•ã‚“)', '', cleaned_text)
    cleaned_text = cleaned_text.strip().rstrip('â¤ï¸ğŸ©·') + 'ğŸ©·'
    return cleaned_text

def format_reply(text: str, lang: str = 'ja') -> str:
    processed_text = text.strip()
    if lang == 'ja':
        # åŸºæœ¬çš„ãªæ”¹è¡Œå‡¦ç†ã®ã¿ï¼šå¥èª­ç‚¹å¾Œã®ã‚¹ãƒšãƒ¼ã‚¹ã¨å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã‚’æ”¹è¡Œã«å¤‰æ›
        processed_text = processed_text.replace('ã€‚ ', 'ã€‚\n').replace('ã€‚ã€€', 'ã€‚\n')
        processed_text = processed_text.replace('ï¼ ', 'ï¼\n').replace('ï¼ã€€', 'ï¼\n')
        processed_text = processed_text.replace('ï¼Ÿ ', 'ï¼Ÿ\n').replace('ï¼Ÿã€€', 'ï¼Ÿ\n')
        processed_text = processed_text.replace('â€¦ ', 'â€¦\n').replace('â€¦ã€€', 'â€¦\n')
        processed_text = processed_text.replace('ã€€', '\n')
        
        # é€£ç¶šã™ã‚‹æ”¹è¡Œã‚’ã¾ã¨ã‚ã‚‹
        processed_text = re.sub(r'\n+', '\n', processed_text)
    return processed_text.strip()

# --- Selenium & BeautifulSoup è§£æé–¢æ•° ---

def _get_tweet_text(article: BeautifulSoup) -> str:
    """è¨˜äº‹è¦ç´ ã‹ã‚‰ãƒ„ã‚¤ãƒ¼ãƒˆæœ¬æ–‡ã‚’å–å¾—ã—ã¾ã™ã€‚"""
    text_div = article.find('div', {'data-testid': 'tweetText'})
    return text_div.get_text(separator=' ', strip=True) if text_div else ""

def _is_tweet_a_reply(article: BeautifulSoup) -> bool:
    """
    è¨˜äº‹è¦ç´ ãŒè¿”ä¿¡ãƒ„ã‚¤ãƒ¼ãƒˆã§ã‚ã‚‹ã‹åˆ¤å®šã—ã¾ã™ã€‚
    è¤‡æ•°ã®åˆ¤å®šæ–¹æ³•ã‚’çµ„ã¿åˆã‚ã›ã¦ç¢ºå®Ÿã«æ¤œå‡ºã—ã¾ã™ã€‚
    """
    # ãƒ„ã‚¤ãƒ¼ãƒˆIDã‚’å–å¾—ã—ã¦ãƒ­ã‚°ã«è¨˜éŒ²
    tweet_id = _extract_tweet_id_for_debug(article)
    author = _get_author_from_article(article)
    
    reply_judge_logger.info(f"=== è¿”ä¿¡åˆ¤å®šé–‹å§‹: ID={tweet_id}, Author={author} ===")
    
    # æ–¹æ³•1: UIä¸Šã®è¿”ä¿¡å…ˆè¡¨ç¤ºãƒ†ã‚­ã‚¹ãƒˆã‚’æ¤œç´¢ï¼ˆå¤šè¨€èªå¯¾å¿œï¼‰
    reply_patterns = [
        r'Replying to',
        r'è¿”ä¿¡å…ˆ:',
        r'è¿”ä¿¡å…ˆï¼š',
        r'Replying to @',
        r'En respuesta a',  # ã‚¹ãƒšã‚¤ãƒ³èª
        r'RÃ©pondre Ã ',      # ãƒ•ãƒ©ãƒ³ã‚¹èª
        r'Antwort an',      # ãƒ‰ã‚¤ãƒ„èª
        r'å›å¤',            # ä¸­å›½èª
        r'ë‹µê¸€',            # éŸ“å›½èª
    ]
    
    reply_judge_logger.debug(f"æ–¹æ³•1: ãƒ†ã‚­ã‚¹ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œç´¢é–‹å§‹")
    for pattern in reply_patterns:
        found = article.find(string=re.compile(pattern, re.IGNORECASE))
        if found:
            reply_judge_logger.info(f"è¿”ä¿¡åˆ¤å®šæˆåŠŸ: ãƒ†ã‚­ã‚¹ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ '{pattern}' ã§æ¤œå‡º")
            reply_judge_logger.debug(f"  æ¤œå‡ºãƒ†ã‚­ã‚¹ãƒˆ: '{found.strip()}'")
            return True
        else:
            reply_judge_logger.debug(f"  ãƒ‘ã‚¿ãƒ¼ãƒ³ '{pattern}': æœªæ¤œå‡º")
    
    # æ–¹æ³•2: data-testidå±æ€§ã«ã‚ˆã‚‹åˆ¤å®šï¼ˆè¿”ä¿¡ãƒœã‚¿ãƒ³ã‚’é™¤å¤–ï¼‰
    reply_indicators = [
        '[data-testid="inReplyTo"]',  # æ­£ç¢ºãªè¿”ä¿¡å…ˆè¡¨ç¤ºã®ã¿
        '[aria-label*="Replying to"]'  # è¿”ä¿¡å…ˆã‚’ç¤ºã™aria-label
    ]
    
    reply_judge_logger.debug(f"æ–¹æ³•2: data-testidå±æ€§æ¤œç´¢é–‹å§‹")
    for selector in reply_indicators:
        elements = article.select(selector)
        if elements:
            reply_judge_logger.info(f"è¿”ä¿¡åˆ¤å®šæˆåŠŸ: ã‚»ãƒ¬ã‚¯ã‚¿ '{selector}' ã§æ¤œå‡º")
            reply_judge_logger.debug(f"  æ¤œå‡ºè¦ç´ æ•°: {len(elements)}")
            for i, elem in enumerate(elements[:3]):  # æœ€åˆã®3ã¤ã‚’è¡¨ç¤º
                reply_judge_logger.debug(f"  è¦ç´ {i+1}: {elem.name} - {elem.get('data-testid', '')} - {elem.get('aria-label', '')}")
            return True
        else:
            reply_judge_logger.debug(f"  ã‚»ãƒ¬ã‚¯ã‚¿ '{selector}': æœªæ¤œå‡º")
    
    # æ–¹æ³•3: URLæ§‹é€ ã«ã‚ˆã‚‹åˆ¤å®š
    reply_judge_logger.debug(f"æ–¹æ³•3: URLæ§‹é€ æ¤œç´¢é–‹å§‹")
    links = article.find_all('a', href=True)
    reply_judge_logger.debug(f"  ç·ãƒªãƒ³ã‚¯æ•°: {len(links)}")
    
    for i, link in enumerate(links):
        href = link['href']
        if '/status/' in href and 'reply' in href.lower():
            reply_judge_logger.info(f"è¿”ä¿¡åˆ¤å®šæˆåŠŸ: URLãƒ‘ã‚¿ãƒ¼ãƒ³ã§æ¤œå‡º '{href}'")
            return True
        elif '/status/' in href:
            reply_judge_logger.debug(f"  ãƒªãƒ³ã‚¯{i+1}: {href} (è¿”ä¿¡ãƒ‘ã‚¿ãƒ¼ãƒ³ãªã—)")
    
    # æ–¹æ³•4: DOMæ§‹é€ ã«ã‚ˆã‚‹åˆ¤å®šï¼ˆãƒ„ã‚¤ãƒ¼ãƒˆæœ¬æ–‡å¤–ã®@ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³æ¤œå‡ºï¼‰
    reply_judge_logger.debug(f"æ–¹æ³•4: DOMæ§‹é€ æ¤œç´¢é–‹å§‹")
    
    # ãƒ„ã‚¤ãƒ¼ãƒˆæœ¬æ–‡è¦ç´ ã‚’ç‰¹å®š
    tweet_text_div = article.find('div', {'data-testid': 'tweetText'})
    
    # ãƒ„ã‚¤ãƒ¼ãƒˆæœ¬æ–‡å¤–ã§ã®ã¿@ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ã‚’æ¤œç´¢
    parent_indicators = []
    for elem in article.find_all(['span', 'div'], string=re.compile(r'@\w+', re.IGNORECASE)):
        # ãƒ„ã‚¤ãƒ¼ãƒˆæœ¬æ–‡å†…ã®@ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ã¯é™¤å¤–
        if tweet_text_div and elem in tweet_text_div.descendants:
            continue
        # "è¿”ä¿¡å…ˆ" "Replying to" ãªã©ã®æ–‡å­—åˆ—ã¨çµ„ã¿åˆã‚ã•ã£ã¦ã„ã‚‹@ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ã®ã¿å¯¾è±¡
        elem_text = elem.get_text().strip()
        parent_text = elem.parent.get_text().strip() if elem.parent else ""
        if any(keyword in parent_text.lower() for keyword in ['replying', 'è¿”ä¿¡', 'respuesta', 'rÃ©pondre', 'antwort', 'å›å¤', 'ë‹µê¸€']):
            parent_indicators.append(elem)
    
    reply_judge_logger.debug(f"  è¿”ä¿¡å…ˆ@ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³è¦ç´ æ•°: {len(parent_indicators)}")
    
    if parent_indicators:
        reply_judge_logger.info(f"è¿”ä¿¡åˆ¤å®šæˆåŠŸ: è¿”ä¿¡å…ˆ@ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³({len(parent_indicators)}å€‹)ã§æ¤œå‡º")
        for i, indicator in enumerate(parent_indicators[:3]):
            reply_judge_logger.debug(f"  è¿”ä¿¡å…ˆ@ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³{i+1}: '{indicator.get_text().strip()}'")
        return True
    
    # æ–¹æ³•5: ãƒ„ã‚¤ãƒ¼ãƒˆæ§‹é€ ã®è©³ç´°åˆ†æï¼ˆã‚ˆã‚Šå³å¯†ãªè¿”ä¿¡å…ˆæ¤œå‡ºï¼‰
    reply_judge_logger.debug(f"æ–¹æ³•5: ãƒ„ã‚¤ãƒ¼ãƒˆæ§‹é€ è©³ç´°åˆ†æé–‹å§‹")
    if tweet_text_div:
        prev_siblings = tweet_text_div.find_all_previous('div')
        reply_judge_logger.debug(f"  ãƒ„ã‚¤ãƒ¼ãƒˆãƒ†ã‚­ã‚¹ãƒˆå‰ã®è¦ç´ æ•°: {len(prev_siblings)}")
        
        for i, sibling in enumerate(prev_siblings[:5]):
            sibling_text = sibling.get_text().strip()
            # @ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ãŒã‚ã‚Šã€ã‹ã¤è¿”ä¿¡é–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚‹å ´åˆã®ã¿
            if (sibling_text and 
                re.search(r'@\w+', sibling_text) and 
                any(keyword in sibling_text.lower() for keyword in ['replying', 'è¿”ä¿¡', 'respuesta', 'rÃ©pondre', 'antwort', 'å›å¤', 'ë‹µê¸€'])):
                reply_judge_logger.info(f"è¿”ä¿¡åˆ¤å®šæˆåŠŸ: è¿”ä¿¡å…ˆæƒ…å ±ã§æ¤œå‡º")
                reply_judge_logger.debug(f"  æ¤œå‡ºè¦ç´ {i+1}: '{sibling_text}'")
                return True
            elif sibling_text:
                reply_judge_logger.debug(f"  è¦ç´ {i+1}: '{sibling_text[:50]}...' (è¿”ä¿¡å…ˆãƒ‘ã‚¿ãƒ¼ãƒ³ãªã—)")
    else:
        reply_judge_logger.debug("  ãƒ„ã‚¤ãƒ¼ãƒˆãƒ†ã‚­ã‚¹ãƒˆè¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    # æ–¹æ³•6: æ–°ã—ã„åˆ¤å®šæ–¹æ³• - articleè¦ç´ ã®classå±æ€§ã‚’ãƒã‚§ãƒƒã‚¯
    reply_judge_logger.debug(f"æ–¹æ³•6: articleè¦ç´ ã®classå±æ€§ãƒã‚§ãƒƒã‚¯")
    article_classes = article.get('class', [])
    reply_judge_logger.debug(f"  articleã‚¯ãƒ©ã‚¹: {article_classes}")
    
    # æ–¹æ³•7: data-testid="tweet"ã®è¦ªè¦ç´ ã‚’ãƒã‚§ãƒƒã‚¯
    reply_judge_logger.debug(f"æ–¹æ³•7: è¦ªè¦ç´ æ§‹é€ ãƒã‚§ãƒƒã‚¯")
    parent = article.parent
    if parent:
        parent_attrs = {k: v for k, v in parent.attrs.items() if k in ['class', 'data-testid', 'role']}
        reply_judge_logger.debug(f"  è¦ªè¦ç´ å±æ€§: {parent_attrs}")
    
    reply_judge_logger.info(f"è¿”ä¿¡åˆ¤å®šçµæœ: éè¿”ä¿¡ãƒ„ã‚¤ãƒ¼ãƒˆã¨åˆ¤å®š (ID={tweet_id})")
    return False

def _extract_tweet_id_for_debug(article: BeautifulSoup) -> str:
    """ãƒ‡ãƒãƒƒã‚°ç”¨ã®ãƒ„ã‚¤ãƒ¼ãƒˆIDæŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
    link = article.find('a', href=lambda href: href and '/status/' in href)
    if link and 'href' in link.attrs:
        href = link['href']
        if '/status/' in href:
            return href.split('/status/')[-1].split('?')[0]
    return "unknown"

def _get_author_from_article(article: BeautifulSoup) -> str | None:
    """è¨˜äº‹è¦ç´ ã‹ã‚‰æŠ•ç¨¿è€…ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã‚’å–å¾—ã—ã¾ã™ã€‚"""
    user_name_div = article.find('div', {'data-testid': 'User-Name'})
    if user_name_div:
        user_link = user_name_div.find('a', {'role': 'link', 'href': lambda href: href and href.startswith('/') and '/status/' not in href})
        if user_link and 'href' in user_link.attrs:
            return user_link['href'].lstrip('/')
    return None

def _get_live_reply_count(article: BeautifulSoup) -> int:
    """è¨˜äº‹è¦ç´ ã‹ã‚‰ãƒ©ã‚¤ãƒ–ã®è¿”ä¿¡æ•°ã‚’å–å¾—ã—ã¾ã™ã€‚è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯0ã‚’è¿”ã—ã¾ã™ã€‚"""
    try:
        # ãƒ„ã‚¤ãƒ¼ãƒˆãƒ•ãƒƒã‚¿ãƒ¼å†…ã®å„ç¨®çµ±è¨ˆæƒ…å ±ã‚’æ¢ã™
        reply_div = article.find('div', {'data-testid': 'reply'})
        if reply_div:
            # "stat"ã¨ã„ã†data-testidã‚’æŒã¤spanã‹ã‚‰æ•°å€¤ã‚’å–å¾—
            stat_span = reply_div.find('span', {'data-testid': 'stat'})
            if stat_span and stat_span.text.strip().isdigit():
                return int(stat_span.text.strip())
    except (ValueError, AttributeError):
        # ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ã‚„è¦ç´ ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯0ã‚’è¿”ã™
        pass
    return 0

def _get_live_like_count(article: BeautifulSoup) -> int:
    """è¨˜äº‹è¦ç´ ã‹ã‚‰ãƒ©ã‚¤ãƒ–ã®ã€Œã„ã„ã­ã€æ•°ã‚’å–å¾—ã—ã¾ã™ã€‚è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯0ã‚’è¿”ã—ã¾ã™ã€‚"""
    try:
        like_div = article.find('div', {'data-testid': 'like'})
        if like_div:
            stat_span = like_div.find('span', {'data-testid': 'stat'})
            if stat_span and stat_span.text.strip().isdigit():
                return int(stat_span.text.strip())
    except (ValueError, AttributeError):
        pass
    return 0

def _safe_scroll_to_direction(driver: webdriver.Chrome, direction: str, max_attempts: int = 3) -> bool:
    """
    å®‰å…¨ã«ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
    direction: 'up' ã¾ãŸã¯ 'down'
    """
    try:
        for attempt in range(max_attempts):
            if direction == 'up':
                driver.execute_script("window.scrollBy(0, -3000);")
            else:
                driver.execute_script("window.scrollBy(0, 3000);")
            time.sleep(1.5)  # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¾Œã®å¾…æ©Ÿ
        return True
    except Exception as e:
        logging.warning(f"ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def _extract_tweet_data(article: BeautifulSoup) -> dict:
    """
    è¨˜äº‹è¦ç´ ã‹ã‚‰åŸºæœ¬çš„ãªãƒ„ã‚¤ãƒ¼ãƒˆæƒ…å ±ã‚’æŠ½å‡ºã—ã¾ã™ã€‚
    """
    try:
        # ãƒ„ã‚¤ãƒ¼ãƒˆIDã®æŠ½å‡ºï¼ˆè¤‡æ•°ã®æ–¹æ³•ã‚’è©¦è¡Œï¼‰
        tweet_id = None
        
        # æ–¹æ³•1: hrefå±æ€§ã‹ã‚‰æŠ½å‡º
        link = article.find('a', href=lambda href: href and '/status/' in href)
        if link and 'href' in link.attrs:
            href = link['href']
            if '/status/' in href:
                tweet_id = href.split('/status/')[-1].split('?')[0]
                thread_debug_logger.debug(f"ãƒ„ã‚¤ãƒ¼ãƒˆIDæŠ½å‡ºæ–¹æ³•1æˆåŠŸ: {tweet_id}")
        
        # æ–¹æ³•2: aria-labelbackupã‹ã‚‰ã®æŠ½å‡ºï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if not tweet_id:
            time_element = article.find('time')
            if time_element and time_element.parent:
                parent_link = time_element.parent
                if parent_link.name == 'a' and 'href' in parent_link.attrs:
                    href = parent_link['href']
                    if '/status/' in href:
                        tweet_id = href.split('/status/')[-1].split('?')[0]
                        thread_debug_logger.debug(f"ãƒ„ã‚¤ãƒ¼ãƒˆIDæŠ½å‡ºæ–¹æ³•2æˆåŠŸ: {tweet_id}")
        
        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®æŠ½å‡º
        timestamp = None
        time_element = article.find('time')
        if time_element and 'datetime' in time_element.attrs:
            timestamp = time_element['datetime']
        
        author = _get_author_from_article(article)
        text = _get_tweet_text(article)
        is_reply = _is_tweet_a_reply(article)
        
        # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°å‡ºåŠ›
        thread_debug_logger.debug(f"ãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿æŠ½å‡ºçµæœ:")
        thread_debug_logger.debug(f"  ID: {tweet_id}")
        thread_debug_logger.debug(f"  ä½œè€…: {author}")
        thread_debug_logger.debug(f"  ãƒ†ã‚­ã‚¹ãƒˆ: {text[:50]}{'...' if len(text) > 50 else ''}")
        thread_debug_logger.debug(f"  ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—: {timestamp}")
        thread_debug_logger.debug(f"  è¿”ä¿¡ãƒ•ãƒ©ã‚°: {is_reply}")
        
        return {
            "tweet_id": tweet_id,
            "author": author,
            "text": text,
            "timestamp": timestamp,
            "is_reply": is_reply,
            "article": article
        }
    except Exception as e:
        thread_debug_logger.error(f"ãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡ºã§ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        return None

def _get_complete_thread(driver: webdriver.Chrome, target_tweet_id: str) -> dict:
    """
    ã‚¹ãƒ¬ãƒƒãƒ‰å…¨ä½“ã‚’ç¢ºå®Ÿã«å–å¾—ã™ã‚‹å …ç‰¢ãªå®Ÿè£…ã€‚
    å…ˆé ­ãƒ»æœ«å°¾ãƒ»æ™‚ç³»åˆ—é †åºã‚’ä¿è¨¼ã—ã¾ã™ã€‚
    """
    try:
        thread_debug_logger.info(f"=== ã‚¹ãƒ¬ãƒƒãƒ‰å…¨ä½“ã®å–å¾—é–‹å§‹ (target_id: {target_tweet_id}) ===")
        
        # åˆæœŸçŠ¶æ…‹ã®ãƒšãƒ¼ã‚¸ã‚½ãƒ¼ã‚¹ã‚µã‚¤ã‚ºã‚’è¨˜éŒ²
        initial_page_size = len(driver.page_source)
        thread_debug_logger.debug(f"åˆæœŸãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚º: {initial_page_size} æ–‡å­—")
        
        # 1. ä¸Šæ–¹å‘ã«ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦å…ˆé ­ã‚’æ¢ã™
        thread_debug_logger.info("=== å…ˆé ­ã‚’æ¢ã™ãŸã‚ä¸Šæ–¹å‘ã«ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«é–‹å§‹ ===")
        prev_page_size = initial_page_size
        up_scroll_count = 0
        max_up_scrolls = 10  # æœ€å¤§ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å›æ•°ã‚’å¢—åŠ 
        
        while up_scroll_count < max_up_scrolls:
            _safe_scroll_to_direction(driver, 'up', 1)
            time.sleep(2)
            current_page_size = len(driver.page_source)
            
            thread_debug_logger.debug(f"ä¸Šã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«{up_scroll_count + 1}å›ç›®: ãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚º {prev_page_size} -> {current_page_size}")
            
            # ãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚ºãŒå¤‰ã‚ã‚‰ãªã‘ã‚Œã°ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å®Œäº†
            if current_page_size == prev_page_size:
                thread_debug_logger.info(f"ä¸Šã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å®Œäº†: {up_scroll_count + 1}å›ã§ãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚ºå¤‰åŒ–ãªã—")
                break
            
            prev_page_size = current_page_size
            up_scroll_count += 1
        
        # 2. ä¸‹æ–¹å‘ã«ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦æœ«å°¾ã‚’æ¢ã™
        thread_debug_logger.info("=== æœ«å°¾ã‚’æ¢ã™ãŸã‚ä¸‹æ–¹å‘ã«ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«é–‹å§‹ ===")
        down_scroll_count = 0
        max_down_scrolls = 10  # æœ€å¤§ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å›æ•°ã‚’å¢—åŠ 
        
        while down_scroll_count < max_down_scrolls:
            _safe_scroll_to_direction(driver, 'down', 1)
            time.sleep(2)
            current_page_size = len(driver.page_source)
            
            thread_debug_logger.debug(f"ä¸‹ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«{down_scroll_count + 1}å›ç›®: ãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚º {prev_page_size} -> {current_page_size}")
            
            # ãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚ºãŒå¤‰ã‚ã‚‰ãªã‘ã‚Œã°ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å®Œäº†
            if current_page_size == prev_page_size:
                thread_debug_logger.info(f"ä¸‹ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å®Œäº†: {down_scroll_count + 1}å›ã§ãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚ºå¤‰åŒ–ãªã—")
                break
            
            prev_page_size = current_page_size
            down_scroll_count += 1
        
        # 3. ç¾åœ¨ã®ãƒšãƒ¼ã‚¸çŠ¶æ…‹ã§ãƒ„ã‚¤ãƒ¼ãƒˆã‚’å–å¾—
        thread_debug_logger.info("=== ãƒšãƒ¼ã‚¸è§£æé–‹å§‹ ===")
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        # è¤‡æ•°ã‚»ãƒ¬ã‚¯ã‚¿ã§ç¢ºå®Ÿã«ãƒ„ã‚¤ãƒ¼ãƒˆè¦ç´ ã‚’å–å¾—
        tweet_selectors = [
            'article[data-testid="tweet"]',
            'div[data-testid="tweet"]',
            '[role="article"]'
        ]
        
        all_articles = []
        for selector in tweet_selectors:
            articles = soup.select(selector)
            thread_debug_logger.debug(f"ã‚»ãƒ¬ã‚¯ã‚¿ '{selector}' ã§ {len(articles)} ä»¶ã®è¦ç´ ç™ºè¦‹")
            if articles:
                all_articles = articles
                break
        
        if not all_articles:
            thread_debug_logger.error("ãƒ„ã‚¤ãƒ¼ãƒˆè¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return None
        
        thread_debug_logger.info(f"åˆè¨ˆ {len(all_articles)} ä»¶ã®ãƒ„ã‚¤ãƒ¼ãƒˆè¦ç´ ã‚’ç™ºè¦‹")
        
        # 4. å„ãƒ„ã‚¤ãƒ¼ãƒˆã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        thread_debug_logger.info("=== ãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿æŠ½å‡ºé–‹å§‹ ===")
        tweet_data_list = []
        for i, article in enumerate(all_articles):
            thread_debug_logger.debug(f"--- è¨˜äº‹è¦ç´  {i+1}/{len(all_articles)} ã‚’å‡¦ç†ä¸­ ---")
            tweet_data = _extract_tweet_data(article)
            if tweet_data and tweet_data["tweet_id"]:
                tweet_data_list.append(tweet_data)
                thread_debug_logger.debug(f"æœ‰åŠ¹ãªãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦è¿½åŠ : {tweet_data['tweet_id']}")
            else:
                thread_debug_logger.debug("ç„¡åŠ¹ãªãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
        
        if not tweet_data_list:
            thread_debug_logger.error("æœ‰åŠ¹ãªãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
            return None
        
        thread_debug_logger.info(f"æœ‰åŠ¹ãªãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ {len(tweet_data_list)} ä»¶ã‚’æŠ½å‡º")
        
        # 5. ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§ã‚½ãƒ¼ãƒˆï¼ˆæ™‚ç³»åˆ—é †åºä¿è¨¼ï¼‰
        thread_debug_logger.info("=== æ™‚ç³»åˆ—ã‚½ãƒ¼ãƒˆé–‹å§‹ ===")
        valid_tweets = [t for t in tweet_data_list if t["timestamp"]]
        thread_debug_logger.debug(f"ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æœ‰ã‚Šãƒ„ã‚¤ãƒ¼ãƒˆ: {len(valid_tweets)} ä»¶")
        thread_debug_logger.debug(f"ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ç„¡ã—ãƒ„ã‚¤ãƒ¼ãƒˆ: {len(tweet_data_list) - len(valid_tweets)} ä»¶")
        
        if valid_tweets:
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§ã‚½ãƒ¼ãƒˆå‰ã®é †åºã‚’ãƒ­ã‚°å‡ºåŠ›
            thread_debug_logger.debug("ã‚½ãƒ¼ãƒˆå‰ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—é †åº:")
            for i, tweet in enumerate(valid_tweets):
                thread_debug_logger.debug(f"  {i}: {tweet['timestamp']} - {tweet['tweet_id']} - @{tweet['author']}")
            
            valid_tweets.sort(key=lambda x: x["timestamp"])
            timeline = valid_tweets
            
            # ã‚½ãƒ¼ãƒˆå¾Œã®é †åºã‚’ãƒ­ã‚°å‡ºåŠ›
            thread_debug_logger.debug("ã‚½ãƒ¼ãƒˆå¾Œã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—é †åº:")
            for i, tweet in enumerate(timeline):
                thread_debug_logger.debug(f"  {i}: {tweet['timestamp']} - {tweet['tweet_id']} - @{tweet['author']}")
        else:
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãŒå–å¾—ã§ããªã„å ´åˆã¯DOMé †åºã‚’ç¶­æŒ
            thread_debug_logger.warning("ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãŒå–å¾—ã§ããªã„ãŸã‚ã€DOMé †åºã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            timeline = tweet_data_list
            
            # DOMé †åºã‚’ãƒ­ã‚°å‡ºåŠ›
            thread_debug_logger.debug("DOMé †åº:")
            for i, tweet in enumerate(timeline):
                thread_debug_logger.debug(f"  {i}: {tweet['tweet_id']} - @{tweet['author']}")
        
        # 6. å…ˆé ­ã¨æœ«å°¾ã®ç‰¹å®š
        thread_debug_logger.info("=== å…ˆé ­ãƒ»æœ«å°¾ã®ç‰¹å®šé–‹å§‹ ===")
        thread_head = None
        thread_tail = None
        
        if timeline:
            # å…ˆé ­: ã€Œè¿”ä¿¡å…ˆã€è¡¨ç¤ºãŒãªã„æœ€åˆã®ãƒ„ã‚¤ãƒ¼ãƒˆ
            for i, tweet_data in enumerate(timeline):
                thread_debug_logger.debug(f"å…ˆé ­å€™è£œ {i}: ID={tweet_data['tweet_id']}, is_reply={tweet_data['is_reply']}, author=@{tweet_data['author']}")
                if not tweet_data["is_reply"]:
                    thread_head = tweet_data
                    thread_debug_logger.info(f"å…ˆé ­ç¢ºå®š: {tweet_data['tweet_id']} - @{tweet_data['author']}")
                    break
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: è¿”ä¿¡ã§ãªã„ãƒ„ã‚¤ãƒ¼ãƒˆãŒãªã„å ´åˆã¯æœ€åˆã®ãƒ„ã‚¤ãƒ¼ãƒˆ
            if not thread_head:
                thread_head = timeline[0]
                thread_debug_logger.warning(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å…ˆé ­: {thread_head['tweet_id']} - @{thread_head['author']}")
            
            # æœ«å°¾: æ™‚ç³»åˆ—é †ã§æœ€å¾Œã®ãƒ„ã‚¤ãƒ¼ãƒˆ
            thread_tail = timeline[-1]
            thread_debug_logger.info(f"æœ«å°¾ç¢ºå®š: {thread_tail['tweet_id']} - @{thread_tail['author']}")
        
        result = {
            "head": thread_head,
            "tail": thread_tail,
            "timeline": timeline,
            "total_tweets": len(timeline)
        }
        
        # è©³ç´°ãªã‚µãƒãƒªãƒ¼ãƒ­ã‚°
        thread_debug_logger.info("=== ã‚¹ãƒ¬ãƒƒãƒ‰è§£æå®Œäº†ã‚µãƒãƒªãƒ¼ ===")
        thread_debug_logger.info(f"ç·ãƒ„ã‚¤ãƒ¼ãƒˆæ•°: {len(timeline)}")
        thread_debug_logger.info(f"å…ˆé ­ãƒ„ã‚¤ãƒ¼ãƒˆ: {thread_head['author'] if thread_head else 'N/A'} (ID: {thread_head['tweet_id'] if thread_head else 'N/A'})")
        thread_debug_logger.info(f"æœ«å°¾ãƒ„ã‚¤ãƒ¼ãƒˆ: {thread_tail['author'] if thread_tail else 'N/A'} (ID: {thread_tail['tweet_id'] if thread_tail else 'N/A'})")
        thread_debug_logger.info(f"ä¸Šã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å›æ•°: {up_scroll_count}")
        thread_debug_logger.info(f"ä¸‹ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å›æ•°: {down_scroll_count}")
        
        # å…¨ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦å‡ºåŠ›ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        debug_filename = f"log/thread_timeline_{target_tweet_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        debug_data = {
            "target_tweet_id": target_tweet_id,
            "total_tweets": len(timeline),
            "head_tweet_id": thread_head['tweet_id'] if thread_head else None,
            "tail_tweet_id": thread_tail['tweet_id'] if thread_tail else None,
            "up_scroll_count": up_scroll_count,
            "down_scroll_count": down_scroll_count,
            "timeline": [
                {
                    "tweet_id": t['tweet_id'],
                    "author": t['author'],
                    "text": t['text'][:100] + ('...' if len(t['text']) > 100 else ''),
                    "timestamp": t['timestamp'],
                    "is_reply": t['is_reply']
                }
                for t in timeline
            ]
        }
        
        with open(debug_filename, 'w', encoding='utf-8') as f:
            json.dump(debug_data, f, ensure_ascii=False, indent=2)
        thread_debug_logger.info(f"è©³ç´°ãªã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’ {debug_filename} ã«å‡ºåŠ›ã—ã¾ã—ãŸ")
        
        logging.info(f"ã‚¹ãƒ¬ãƒƒãƒ‰è§£æå®Œäº†: å…¨{len(timeline)}ãƒ„ã‚¤ãƒ¼ãƒˆã€å…ˆé ­={thread_head['author'] if thread_head else 'N/A'}ã€æœ«å°¾={thread_tail['author'] if thread_tail else 'N/A'}")
        return result
        
    except Exception as e:
        thread_debug_logger.error(f"ã‚¹ãƒ¬ãƒƒãƒ‰å…¨ä½“å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        logging.error(f"ã‚¹ãƒ¬ãƒƒãƒ‰å…¨ä½“å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        return None

def fetch_and_analyze_thread(tweet_id: str, driver: webdriver.Chrome) -> dict:
    """
    æŒ‡å®šã•ã‚ŒãŸtweet_idã®ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã€ã‚¹ãƒ¬ãƒƒãƒ‰å…¨ä½“ã‚’è§£æã—ã¦å¿…è¦ãªæƒ…å ±ã‚’è¿”ã—ã¾ã™ã€‚
    æ”¹è‰¯ç‰ˆ: å…ˆé ­ãƒ»æœ«å°¾ãƒ»æ™‚ç³»åˆ—å…¨ä½“ã‚’ç¢ºå®Ÿã«å–å¾—ã—ã¾ã™ã€‚
    """
    tweet_url = f"https://x.com/any/status/{tweet_id}"
    result = {
        "should_skip": True, "is_my_thread": False, "conversation_history": [],
        "current_reply_text": "", "current_replier_id": None, "lang": "und",
        "live_reply_count": 0, "live_like_count": 0,
        "thread_head": None, "thread_tail": None, "full_timeline": []
    }
    
    try:
        driver.get(tweet_url)
        WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.XPATH, '//article[@data-testid="tweet"]')))
        
        # ã‚¹ãƒ¬ãƒƒãƒ‰å…¨ä½“ã‚’å–å¾—ã™ã‚‹
        thread_data = _get_complete_thread(driver, tweet_id)
        if not thread_data:
            logging.warning("ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            return result

        # å¯¾è±¡ãƒ„ã‚¤ãƒ¼ãƒˆã®æ¤œç´¢
        target_article = None
        target_index = -1
        for i, tweet_data in enumerate(thread_data["timeline"]):
            if tweet_data["tweet_id"] == tweet_id:
                target_article = tweet_data["article"]
                target_index = i
                break
        
        if not target_article:
            logging.error("è¿”ä¿¡å¯¾è±¡ã®ãƒ„ã‚¤ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return result

        # ã‚¹ãƒ¬ãƒƒãƒ‰æƒ…å ±ã‚’çµæœã«è¨­å®š
        result["thread_head"] = thread_data["head"]
        result["thread_tail"] = thread_data["tail"] 
        result["full_timeline"] = thread_data["timeline"]
        
        # åŸºæœ¬æƒ…å ±ã®å–å¾—
        live_reply_num = _get_live_reply_count(target_article)
        live_like_num = _get_live_like_count(target_article)
        result["live_reply_count"] = live_reply_num
        result["live_like_count"] = live_like_num
        
        # ã‚¹ãƒ¬ãƒƒãƒ‰ä¸»åˆ¤å®šï¼ˆç¢ºå®Ÿãªå…ˆé ­æƒ…å ±ã‚’ä½¿ç”¨ï¼‰
        head_author = thread_data["head"]["author"] if thread_data["head"] else None
        live_is_my_thread = (head_author == TARGET_USER)
        result["is_my_thread"] = live_is_my_thread
        
        # ã‚¹ãƒ¬ãƒƒãƒ‰ä¸»åˆ¤å®šã®è©³ç´°ãƒ­ã‚°
        thread_owner_logger.info(f"=== ã‚¹ãƒ¬ãƒƒãƒ‰ä¸»åˆ¤å®šè©³ç´° (tweet_id: {tweet_id}) ===")
        thread_owner_logger.info(f"å…ˆé ­ä½œè€…: '{head_author}' (å‹: {type(head_author)})")
        thread_owner_logger.info(f"TARGET_USER: '{TARGET_USER}' (å‹: {type(TARGET_USER)})")
        thread_owner_logger.info(f"æ¯”è¼ƒçµæœ: {head_author == TARGET_USER}")
        thread_owner_logger.info(f"live_is_my_thread: {live_is_my_thread}")
        
        # æ–‡å­—åˆ—æ¯”è¼ƒã®è©³ç´°ãƒã‚§ãƒƒã‚¯
        if head_author and TARGET_USER:
            thread_owner_logger.debug(f"head_author.strip() == TARGET_USER.strip(): {head_author.strip() == TARGET_USER.strip()}")
            thread_owner_logger.debug(f"head_authoré•·ã•: {len(head_author)}, TARGET_USERé•·ã•: {len(TARGET_USER)}")
            thread_owner_logger.debug(f"head_author repr: {repr(head_author)}")
            thread_owner_logger.debug(f"TARGET_USER repr: {repr(TARGET_USER)}")
        
        # ã‚¹ãƒ¬ãƒƒãƒ‰ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³è©³ç´°
        thread_owner_logger.info(f"ã‚¹ãƒ¬ãƒƒãƒ‰ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³è©³ç´°:")
        for i, tweet_info in enumerate(thread_data["timeline"]):
            is_target_user = tweet_info["author"] == TARGET_USER
            thread_owner_logger.info(f"  {i+1}: {tweet_info['tweet_id']} - @{tweet_info['author']} (TARGET_USER: {is_target_user}, is_reply: {tweet_info['is_reply']})")
        
        logging.info(f"ã‚¹ãƒ¬ãƒƒãƒ‰ä¸»åˆ¤å®š: head_author='{head_author}', TARGET_USER='{TARGET_USER}', is_my_thread={live_is_my_thread}")
        process_logger.info(f"Tweet {tweet_id}: is_my_thread={live_is_my_thread}, head_author={head_author}")
        
        # å¾Œç¶šè¿”ä¿¡ã®å­˜åœ¨ç¢ºèªï¼ˆæ™‚ç³»åˆ—ã§ã®ä½ç½®ã‚’ç¢ºèªï¼‰
        has_future_replies = target_index < len(thread_data["timeline"]) - 1
        is_priority_reply = live_is_my_thread and live_reply_num == 0
        
        if has_future_replies and not is_priority_reply:
            num_future_replies = len(thread_data["timeline"]) - (target_index + 1)
            logging.warning(
                f"å¯¾è±¡ãƒ„ã‚¤ãƒ¼ãƒˆã®å¾Œã« {num_future_replies} ä»¶ã®è¿”ä¿¡ãŒã‚ã‚Šã€"
                f"ã‹ã¤å„ªå…ˆè¿”ä¿¡ï¼ˆã‚¹ãƒ¬ä¸»: {live_is_my_thread}, reply_num={live_reply_num}ï¼‰"
                "ã®æ¡ä»¶ã‚’æº€ãŸã•ãªã„ãŸã‚ã€å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"
            )
            return result

        # ä¼šè©±å±¥æ­´ã®æ§‹ç¯‰ï¼ˆå…ˆé ­ã‹ã‚‰å¯¾è±¡ãƒ„ã‚¤ãƒ¼ãƒˆã¾ã§ï¼‰
        result["should_skip"] = False
        conversation_history = []
        for tweet_data in thread_data["timeline"][:target_index + 1]:
            author = tweet_data["author"]
            text = tweet_data["text"]
            conversation_history.append(f"@{author}: {text}")
        result["conversation_history"] = conversation_history
        
        # å¯¾è±¡ãƒ„ã‚¤ãƒ¼ãƒˆæƒ…å ±
        result["current_reply_text"] = _get_tweet_text(target_article)
        result["current_replier_id"] = _get_author_from_article(target_article)
        
        # è¨€èªåˆ¤å®š
        try:
            from langdetect import detect, LangDetectException
            result["lang"] = detect(result["current_reply_text"])
        except (LangDetectException, ImportError):
            result["lang"] = "und"

        return result

    except TimeoutException:
        logging.error(f"ãƒšãƒ¼ã‚¸ã®ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ: {tweet_url}")
        return result
    except Exception as e:
        logging.error(f"ã‚¹ãƒ¬ãƒƒãƒ‰è§£æä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        return result

# --- è¿”ä¿¡å“è³ªãƒã‚§ãƒƒã‚¯é–¢æ•° (æ–°è¦è¿½åŠ ) ---
def self_check_reply(
    generated_reply: str,
    thread_data: dict,
    nickname: str | None,
    banned_phrases: set
) -> Tuple[bool, str]:
    """
    ç”Ÿæˆã•ã‚ŒãŸè¿”ä¿¡ãŒå“è³ªåŸºæº–ã‚’æº€ãŸã—ã¦ã„ã‚‹ã‹ã‚»ãƒ«ãƒ•ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã€‚
    """
    # ãƒã‚§ãƒƒã‚¯1: ç©ºæ–‡å­—åˆ—ã§ãªã„ã‹
    if not generated_reply or not generated_reply.strip():
        return False, "ç”Ÿæˆã•ã‚ŒãŸè¿”ä¿¡ãŒç©ºã§ã™ã€‚"

    # ãƒã‚§ãƒƒã‚¯2: ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆæœ«å°¾ã®çµµæ–‡å­—ï¼‰
    if not generated_reply.strip().endswith('ğŸ©·'):
        return False, f"è¿”ä¿¡ã®æœ«å°¾ã«æ„å›³ã—ãŸçµµæ–‡å­—('ğŸ©·')ãŒä»˜ã„ã¦ã„ã¾ã›ã‚“: {generated_reply}"

    # ãƒã‚§ãƒƒã‚¯3: ãƒ‹ãƒƒã‚¯ãƒãƒ¼ãƒ 
    if nickname and not generated_reply.startswith(nickname):
        return False, f"ãƒ‹ãƒƒã‚¯ãƒãƒ¼ãƒ  '{nickname}' ãŒè¿”ä¿¡ã®å†’é ­ã«å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“: {generated_reply}"

    # ãƒã‚§ãƒƒã‚¯4: ç¦æ­¢ãƒ•ãƒ¬ãƒ¼ã‚º
    # ãƒ‹ãƒƒã‚¯ãƒãƒ¼ãƒ ã‚’é™¤ã„ãŸæœ¬æ–‡ã®ã¿ã‚’ãƒã‚§ãƒƒã‚¯å¯¾è±¡ã¨ã™ã‚‹
    reply_body = generated_reply.replace(f"{nickname}\n", "") if nickname else generated_reply
    for phrase in banned_phrases:
        if phrase in reply_body:
            return False, f"ç¦æ­¢ãƒ•ãƒ¬ãƒ¼ã‚º '{phrase}' ãŒå«ã¾ã‚Œã¦ã„ã¾ã™: {reply_body}"

    # ãƒã‚§ãƒƒã‚¯5: è¨€èªä¸€è²«æ€§
    expected_lang = thread_data.get("lang", "und")
    if expected_lang == 'ja':
        # æ—¥æœ¬èªã®æ–‡è„ˆã§å¤–å›½èªãŒæ··å…¥ã—ã¦ã„ãªã„ã‹ãƒã‚§ãƒƒã‚¯
        foreign_words = re.findall(r'\b(?:Gracias|Thanks?|Hello|Goodbye|Merci|Danke|Ciao)\b', reply_body, re.IGNORECASE)
        if foreign_words:
            return False, f"æ—¥æœ¬èªã®æ–‡è„ˆã§å¤–å›½èª '{', '.join(foreign_words)}' ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ: {reply_body}"
        
        try:
            from langdetect import detect, LangDetectException
            detected_lang = detect(reply_body)
            if detected_lang != 'ja':
                return False, f"æœŸå¾…ã•ã‚Œã‚‹è¨€èª 'ja' ã¨ç•°ãªã‚‹è¨€èª '{detected_lang}' ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ: {reply_body}"
        except (LangDetectException, ImportError):
            logging.warning("è¨€èªæ¤œå‡ºãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒãªã„ã‹ã€è¨€èªåˆ¤å®šã«å¤±æ•—ã—ã¾ã—ãŸã€‚è¨€èªãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")


    # ãƒã‚§ãƒƒã‚¯6: AIã«ã‚ˆã‚‹è‡ªå·±è©•ä¾¡
    try:
        self_check_prompt = (
            f"ã‚ãªãŸã¯ã€ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«åŸºã¥ã„ã¦æ–‡ç« ã‚’ç”Ÿæˆã™ã‚‹AIã§ã™ã€‚\n\n"
            f"--- ãƒ«ãƒ¼ãƒ« ---\n{MAYA_PERSONALITY_PROMPT}\n{REPLY_RULES_PROMPT}\n\n"
            f"--- ç”Ÿæˆã•ã‚ŒãŸæ–‡ç«  ---\n{reply_body}\n\n"
            f"--- è³ªå• ---\nä¸Šè¨˜ã®ã€Œç”Ÿæˆã•ã‚ŒãŸæ–‡ç« ã€ã¯ã€ã‚ãªãŸè‡ªèº«ãŒå®šã‚ãŸä¸Šè¨˜ã®ã€Œãƒ«ãƒ¼ãƒ«ã€ã‚’ã™ã¹ã¦éµå®ˆã—ã¦ã„ã¾ã™ã‹ï¼Ÿ\n"
            f"Yesã‹Noã‹ã®ã¿ã§ã€ç†ç”±ã‚’ä»˜ã‘ãšã«ç­”ãˆã¦ãã ã•ã„ã€‚"
        )
        model = genai.GenerativeModel(GEMINI_MODEL_NAME)
        response = model.generate_content(self_check_prompt)
        
        # å›ç­”ãŒ 'yes' (å°æ–‡å­—ã€ãƒˆãƒªãƒ ) ã§å§‹ã¾ã‚‰ãªã„å ´åˆã¯NG
        if not response.text.strip().lower().startswith('yes'):
            return False, f"AIã«ã‚ˆã‚‹è‡ªå·±è©•ä¾¡ã§å•é¡Œã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚AIã®å›ç­”: {response.text}"

    except Exception as e:
        logging.error(f"AIè‡ªå·±è©•ä¾¡ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        # è‡ªå·±è©•ä¾¡ã§ã‚¨ãƒ©ãƒ¼ãŒèµ·ããŸå ´åˆã¯ã€ãƒã‚§ãƒƒã‚¯ã‚’ãƒ‘ã‚¹ã•ã›ã‚‹ï¼ˆãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ï¼‰
        pass

    return True, "ã™ã¹ã¦ã®ãƒã‚§ãƒƒã‚¯ã‚’é€šéã—ã¾ã—ãŸã€‚"


# --- è¿”ä¿¡ç”Ÿæˆãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯ ---

def generate_reply(thread_data: dict, history: list) -> str:
    """
    è§£æã•ã‚ŒãŸã‚¹ãƒ¬ãƒƒãƒ‰æƒ…å ±ã«åŸºã¥ãã€é©åˆ‡ãªè¿”ä¿¡æ–‡ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
    ã“ã®é–¢æ•°ãŒå‘¼ã°ã‚Œã‚‹æ™‚ç‚¹ã§ã€è¿”ä¿¡å¯¾è±¡ã§ã‚ã‚‹ã“ã¨ã¯ç¢ºå®šã—ã¦ã„ã‚‹å‰æã€‚
    """
    reply_text = thread_data["current_reply_text"]
    replier_id = thread_data["current_replier_id"]
    lang = thread_data["lang"]
    conversation = "\n".join(thread_data["conversation_history"])

    # ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ç­‰ã‚’é™¤å»ã—ãŸã‚¯ãƒªãƒ¼ãƒ³ãªãƒ†ã‚­ã‚¹ãƒˆ
    cleaned_reply_text = re.sub(r'@[\w_]+', '', reply_text).strip()
    cleaned_reply_text = re.sub(r'^[â€¦,:ãƒ»ã€ã€‚]', '', cleaned_reply_text).strip()

    # ãƒ‹ãƒƒã‚¯ãƒãƒ¼ãƒ ã®æœ‰ç„¡ã‚’å…ˆã«å–å¾—
    preference = get_user_preference(replier_id.lower()) if replier_id else None
    nickname = preference[0] if preference else None

    # 1. å®šå‹æ–‡ã§ã®è¿”ä¿¡ï¼ˆãƒ‹ãƒƒã‚¯ãƒãƒ¼ãƒ ãŒãªã„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«é™å®šï¼‰
    if ("ãŠã¯ã‚ˆã†" in cleaned_reply_text or "ãŠã¯ã‚ˆãƒ¼" in cleaned_reply_text) and not nickname:
        return format_reply(f"ãŠã¯ã‚ˆã†{random.choice(['â¤ï¸', 'ğŸ©·'])}", lang)
    if "ã“ã‚“ã«ã¡ã¯" in cleaned_reply_text and not nickname:
        return format_reply(f"ã“ã‚“ã«ã¡ã¯{random.choice(['â¤ï¸', 'ğŸ©·'])}", lang)
    if "ã“ã‚“ã°ã‚“ã¯" in cleaned_reply_text and not nickname:
        return format_reply(f"ã“ã‚“ã°ã‚“ã¯{random.choice(['â¤ï¸', 'ğŸ©·'])}", lang)
    
    # çµµæ–‡å­—ã®ã¿ã€ã¾ãŸã¯çŸ­ã„å¤–å›½èªã®ãƒ„ã‚¤ãƒ¼ãƒˆã«å¯¾ã™ã‚‹å¿œç­”ã‚’æ”¹å–„
    if is_emoji_only(cleaned_reply_text) or (lang != "ja" and len(cleaned_reply_text) <= 15):
        # qmeï¼ˆçµµæ–‡å­—ã®ã¿ï¼‰ã®å ´åˆã€è¨€èªã‚³ãƒ¼ãƒ‰ã¨ã—ã¦'qme'ã‚’ä½¿ç”¨ã™ã‚‹
        lang_code = 'qme' if is_emoji_only(cleaned_reply_text) else lang
        return random.choice(THANK_YOU_PHRASES.get(lang_code, ["ğŸ©·"]))

    # 2. AIã«ã‚ˆã‚‹è¿”ä¿¡
    if lang == "ja" and not nickname and len(cleaned_reply_text) <= 15:
        return random.choice(["ã‚ã‚ŠãŒã¨ã†ğŸ©·", "å¬‰ã—ã„ãªğŸ©·", "ãˆã¸ã¸ã€ç…§ã‚Œã¡ã‚ƒã†ãªğŸ©·", "ãµãµã£ğŸ©·", "ã†ã‚“ã†ã‚“ğŸ©·", "ã‚ãƒ¼ã„ğŸ©·"])

    # --- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ ---
    logging.info(f"AIã¸ã®å…¥åŠ›ï¼ˆä¼šè©±å±¥æ­´ï¼‰:\n---\n{conversation}\n---")
    prompt_parts = [
        MAYA_PERSONALITY_PROMPT,
        "ã‚ãªãŸã¯ä»¥ä¸‹ã®ä¼šè©±ã«å‚åŠ ã—ã¦ã„ã¾ã™ã€‚æœ€å¾Œã®ãƒ•ã‚¡ãƒ³ã‹ã‚‰ã®ãƒªãƒ—ãƒ©ã‚¤ã«è¿”ä¿¡ã—ã¦ãã ã•ã„ã€‚",
        "--- ã“ã‚Œã¾ã§ã®ä¼šè©± ---",
        conversation,
        "--------------------",
        REPLY_RULES_PROMPT
    ]
    if history:
        history_str = "ã€".join(history)
        
        # å±¥æ­´ã‹ã‚‰ç¦æ­¢ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’å‹•çš„ã«æŠ½å‡º
        banned_phrases = set()
        common_verbs = ["ç…§ã‚Œã‚‹", "ç…§ã‚Œã¡ã‚ƒã†", "å¬‰ã—ã„", "å¬‰ã—ã„ãª", "ã‚ã‚ŠãŒã¨ã†", "é ‘å¼µã‚‹", "ãƒ‰ã‚­ãƒ‰ã‚­", "ã™ã”ã„", "ç´ æ•µ"]
        for reply in history:
            for phrase in common_verbs:
                if phrase in reply:
                    banned_phrases.add(phrase)

        avoidance_prompt = (
            "6. **è¡¨ç¾ã®å¤šæ§˜æ€§**: éå»ã®è¿”ä¿¡ã¨åŒã˜è¡¨ç¾ã®ç¹°ã‚Šè¿”ã—ã¯é¿ã‘ã€Mayaã‚‰ã—ã„è‡ªç„¶ãªçŸ­æ–‡ã§è¿”ä¿¡ã—ã¦ãã ã•ã„ã€‚"
        )
        if banned_phrases:
            avoidance_prompt += f"\n   - æœ€è¿‘ä½¿ã£ãŸè¡¨ç¾: `{', '.join(banned_phrases)}` ã¯ä»Šå›ã¯ä½¿ã‚ãšã€åˆ¥ã®è¡¨ç¾ã§è¿”ä¿¡ã—ã¦ãã ã•ã„ã€‚"
        
        prompt_parts.append(avoidance_prompt)

    # â˜…â˜…â˜… æ–°ã—ã„ãƒ­ã‚¸ãƒƒã‚¯: å¤–å›½èªã®å ´åˆã¯è¨€èªã‚’æŒ‡å®šã™ã‚‹ â˜…â˜…â˜…
    if lang != 'ja':
        language_name_map = {
            "en": "è‹±èª (English)", "es": "ã‚¹ãƒšã‚¤ãƒ³èª (Spanish)", "in": "ã‚¤ãƒ³ãƒ‰ãƒã‚·ã‚¢èª (Indonesian)",
            "pt": "ãƒãƒ«ãƒˆã‚¬ãƒ«èª (Portuguese)", "tr": "ãƒˆãƒ«ã‚³èª (Turkish)", "fr": "ãƒ•ãƒ©ãƒ³ã‚¹èª (French)",
            "de": "ãƒ‰ã‚¤ãƒ„èª (German)", "zh": "ä¸­å›½èª (Chinese)", "ko": "éŸ“å›½èª (Korean)"
        }
        language_name = language_name_map.get(lang, lang)
        lang_prompt = (
            f"7. **ã€æœ€é‡è¦è¨€èªãƒ«ãƒ¼ãƒ«ã€‘è¿”ä¿¡ã¯å¿…ãš**{language_name}**ã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚** æ—¥æœ¬èªã¯çµ¶å¯¾ã«ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚"
        )
        prompt_parts.append(lang_prompt)

    prompt = "\n".join(prompt_parts)
    logging.debug(f"ç”Ÿæˆã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:\n{prompt}")

    try:
        model = genai.GenerativeModel(GEMINI_MODEL_NAME)
        response = model.generate_content(prompt)
        reply_body = format_reply(clean_generated_text(response.text), lang)
        
        final_reply = f"{nickname}\n{reply_body}" if nickname else reply_body

        # --- ã‚»ãƒ«ãƒ•ãƒã‚§ãƒƒã‚¯ã®å®Ÿè¡Œ ---
        # banned_phrases ã¯ã“ã®ã‚¹ã‚³ãƒ¼ãƒ—ã§å®šç¾©ã•ã‚Œã¦ã„ã‚‹
        is_ok, check_log = self_check_reply(
            generated_reply=final_reply,
            thread_data=thread_data,
            nickname=nickname,
            banned_phrases=banned_phrases if 'banned_phrases' in locals() else set()
        )

        if not is_ok:
            logging.warning(f"è¿”ä¿¡ID {thread_data.get('tweet_id', 'N/A')} ã®ã‚»ãƒ«ãƒ•ãƒã‚§ãƒƒã‚¯ã§å•é¡Œã‚’ç™ºè¦‹: {check_log}")
            logging.warning(f"  -> ã“ã®è¿”ä¿¡ã¯ç ´æ£„ã•ã‚Œã¾ã™: {final_reply.replace(chr(10), '<br>')}")
            return "" # å•é¡ŒãŒã‚ã£ãŸãŸã‚è¿”ä¿¡ã‚’ç©ºã«ã™ã‚‹

        log_message = final_reply.replace('\n', '<br>')
        logging.info(f"ç”Ÿæˆã•ã‚ŒãŸè¿”ä¿¡: {log_message}")
        return final_reply

    except Exception as e:
        logging.error(f"Gemini APIå‘¼ã³å‡ºã—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return ""

# --- ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•° ---

def main_process(driver: webdriver.Chrome, input_csv: str, limit: int = None) -> str | None:
    logging.info(f"'{input_csv}' ã®å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
    try:
        df = pd.read_csv(input_csv)
        if limit:
            df = df.head(limit)
            logging.info(f"å‡¦ç†ä»¶æ•°ã‚’ {limit} ä»¶ã«åˆ¶é™ã—ã¾ã—ãŸã€‚")
        df.fillna('', inplace=True)

        generated_replies_history = []
        rows_to_drop = [] # å‰Šé™¤å¯¾è±¡ã®è¡Œã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ ¼ç´

        for index, row in df.iterrows():
            tweet_id = str(row['reply_id'])
            
            # --- ã‚¹ãƒ¬ãƒƒãƒ‰è§£æ ---
            thread_data = fetch_and_analyze_thread(tweet_id, driver)
            thread_data['tweet_id'] = tweet_id # ãƒ­ã‚°å‡ºåŠ›ç”¨ã«IDã‚’è¿½åŠ 

            # å–å¾—ã—ãŸãƒ©ã‚¤ãƒ–æƒ…å ±ã§DataFrameã‚’æ›´æ–°
            df.loc[index, 'reply_num'] = thread_data['live_reply_count']
            df.loc[index, 'like_num'] = thread_data['live_like_count']
            df.loc[index, 'is_my_thread'] = thread_data['is_my_thread']

            # --- è¿”ä¿¡ç”Ÿæˆã®åˆ¤æ–­ ---
            # è‡ªåˆ†ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã§ã€ã‹ã¤ã‚¹ã‚­ãƒƒãƒ—å¯¾è±¡ã§ãªã„å ´åˆã®ã¿è¿”ä¿¡ç”Ÿæˆã‚’è©¦ã¿ã‚‹
            if thread_data and not thread_data["should_skip"] and thread_data.get("is_my_thread", False):
                generated_reply = generate_reply(thread_data, generated_replies_history)
                df.loc[index, 'generated_reply'] = generated_reply
                
                if generated_reply:
                    # ã‚»ãƒ«ãƒ•ãƒã‚§ãƒƒã‚¯ã‚’é€šéã—ã€è¿”ä¿¡ãŒæ­£å¸¸ã«ç”Ÿæˆã•ã‚ŒãŸ
                    reply_body = generated_reply.split('\n')[-1]
                    generated_replies_history.append(reply_body.replace('\n', ' '))
                else:
                    # è¿”ä¿¡ç”Ÿæˆã‚’è©¦ã¿ãŸãŒã€ã‚»ãƒ«ãƒ•ãƒã‚§ãƒƒã‚¯ã§å¤±æ•—ã—ãŸ
                    rows_to_drop.append(index)
            else:
                # ãã‚‚ãã‚‚è¿”ä¿¡å¯¾è±¡å¤–ï¼ˆè‡ªåˆ†ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã§ãªã„ã€ã¾ãŸã¯ã‚¹ã‚­ãƒƒãƒ—å¯¾è±¡ï¼‰
                logging.info(f"  -> Tweet ID {tweet_id} ã¯è¿”ä¿¡ç”Ÿæˆã®å¯¾è±¡å¤–ã§ã™ã€‚")
                df.loc[index, 'generated_reply'] = "" # æ˜ç¤ºçš„ã«ç©ºã«ã—ã¦ãŠã

        # --- å¤±æ•—ã—ãŸè¡Œã®å‡¦ç†ã¨å‡ºåŠ› ---
        base_name = os.path.basename(input_csv)
        name_part = base_name.replace('extracted_tweets_', '')

        if rows_to_drop:
            # å¤±æ•—ã—ãŸè¡Œã‚’æ–°ã—ã„DataFrameã¨ã—ã¦æŠ½å‡ºã—ã€åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            failed_df = df.loc[rows_to_drop].copy()
            failed_output_filename = os.path.join("output", f"failed_selfcheck_{name_part}")
            failed_df.to_csv(failed_output_filename, index=False, encoding='utf-8-sig')
            logging.info(f"ã‚»ãƒ«ãƒ•ãƒã‚§ãƒƒã‚¯ã«å¤±æ•—ã—ãŸ {len(rows_to_drop)} ä»¶ã‚’ {failed_output_filename} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

            # å…ƒã®DataFrameã‹ã‚‰å¤±æ•—ã—ãŸè¡Œã‚’å‰Šé™¤
            df.drop(rows_to_drop, inplace=True)
            logging.info("ãƒ¡ã‚¤ãƒ³ã®å‡¦ç†å¯¾è±¡ã‹ã‚‰ä¸Šè¨˜å¤±æ•—ä»¶æ•°ã‚’é™¤å¤–ã—ã¾ã—ãŸã€‚")


        # --- æ­£å¸¸ãªè¡Œã®å‡ºåŠ›å‡¦ç† ---
        output_filename = os.path.join("output", f"processed_replies_{name_part}")
        
        df.to_csv(output_filename, index=False, encoding='utf-8-sig')
        logging.info(f"--- å…¨ä»¶ã®å‡¦ç†ãŒå®Œäº†ã—ã€{output_filename} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ ---")
        return output_filename

    except FileNotFoundError:
        logging.error(f"å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {input_csv}")
        return None
    except Exception as e:
        logging.error(f"ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        return None

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’è§£æã—ã€æ–‡è„ˆã«å¿œã˜ãŸè¿”ä¿¡ã‚’ç”Ÿæˆã—ã¾ã™ã€‚")
    parser.add_argument("input_csv", help="å…¥åŠ›CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ (extracted_tweets_...csv)")
    parser.add_argument("--limit", type=int, help="å‡¦ç†ã™ã‚‹ãƒªãƒ—ãƒ©ã‚¤ã®æœ€å¤§æ•°")
    args = parser.parse_args()

    driver = None
    try:
        driver = setup_driver(headless=False)
        if driver:
            main_process(driver, args.input_csv, args.limit)
    finally:
        if driver:
            driver.quit()
            logging.info("Selenium WebDriverã‚’çµ‚äº†ã—ã¾ã—ãŸã€‚") 